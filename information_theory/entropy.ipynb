{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0253163",
   "metadata": {},
   "source": [
    "# Surprise\n",
    "- Self-information\n",
    "$$I(x)=\\log\\frac{1}{p(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8417e5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts each cases:\n",
      "[[ 6  1]\n",
      " [ 1 10]\n",
      " [ 7  7]]\n",
      "probability:\n",
      "[[0.8571 0.1429]\n",
      " [0.0909 0.9091]\n",
      " [0.5    0.5   ]]\n",
      "surprise:\n",
      "[[0.2224 2.8074]\n",
      " [3.4594 0.1375]\n",
      " [1.     1.    ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "acc = np.array([\n",
    "    [6,  1],\n",
    "    [1, 10],\n",
    "    [7,  7]\n",
    "])\n",
    "print(f'counts each cases:\\n{acc}')\n",
    "prob = acc / acc.sum(axis=1).reshape(-1, 1)\n",
    "print(f'probability:\\n{prob}')\n",
    "surp = np.log2(1 / prob)\n",
    "print(f'surprise:\\n{surp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c75a6b",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "- Expected surprise\n",
    "- Average self-information\n",
    "- Uncertainty, disorderliness\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(x) &= \\mathbb{E}[I(x)] \\\\\n",
    "     &= \\sum_xp(x)\\log\\frac{1}{p(x)} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ade0575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy:\n",
      " [0.5917 0.4395 1.    ]\n"
     ]
    }
   ],
   "source": [
    "entropy = (prob * surp).sum(axis=1)\n",
    "print('entropy:\\n', entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9a65a",
   "metadata": {},
   "source": [
    "# Cross Entropy\n",
    "- Difference between two probability distributions\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(p, q) &= \\mathbb{E}_p[\\log\\frac{1}{q(x)}] \\\\\n",
    "     &= \\sum_xp(x)\\log\\frac{1}{q(x)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- If p and q are the same distribution\n",
    "$$\n",
    "\\begin{align}\n",
    "H(p, p) &= \\mathbb{E}_p[\\log\\frac{1}{p(x)}] \\\\\n",
    "     &= \\sum_xp(x)\\log\\frac{1}{p(x)} \\\\\n",
    "     &= H(p) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- With the concept of Kullback-Liebler divergence,\n",
    "- The cross-entropy is the sum of the entropy and KL-divergence\n",
    "$$\n",
    "\\begin{align}\n",
    "H(p, q) &= \\mathbb{E}_p[\\log\\frac{1}{q(x)}] \\\\\n",
    "     &= \\sum_xp(x)\\log\\frac{1}{q(x)} \\\\\n",
    "     &= \\sum_xp(x)\\log\\frac{\\red{p(x)}}{q(x)}\\frac{1}{\\red{p(x)}} \\\\\n",
    "     &= \\sum_xp(x)\\left(\\log\\frac{p(x)}{q(x)}+\\log\\frac{1}{p(x)}\\right) \\\\\n",
    "     &= D_{KL}(p(x)||q(x)) + H(p) \\\\\n",
    "D_{KL}(p(x)||q(x)) &= \\mathbb{E}_p[\\log\\frac{p(x)}{q(x)}] \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
