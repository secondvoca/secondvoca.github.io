<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Proof of relation between MLE and KL divergence</title>
    <style>
        body {
            font-family: 'Noto Sans KR', Arial, sans-serif;
            background: #f6f7f9;
            color: #333;
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 600px;
            margin: 32px auto;
            background: #fff;
            border-radius: 12px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.06);
            padding: 24px;
        }
        h1 {
            font-size: 2rem;
            margin-bottom: 8px;
            color: #2c3e50;
            text-align: center;
        }
        .book-info {
            font-size: 1rem;
            color: #6c757d;
            text-align: center;
            margin-bottom: 24px;
        }
        .review {
            line-height: 1.7;
            font-size: 1.1rem;
            margin-bottom: 24px;
        }
        @media (max-width: 700px) {
            .container {
                margin: 16px;
                padding: 16px;
            }
            h1 {
                font-size: 1.5rem;
            }
            .review {
                font-size: 1rem;
            }
        }
    </style>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({            
            tex2jax: {inlineMath: [['$$','$$'], ['$','$'], ['\\(','\\)']]}            
        });
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
</head>
<body>
    <div class="container">
        <h1>MLE and KL divergence</h1>
        <div class="review">
            <p>최대우도추정(Maximum Likelihood Estimation, MLE)은 모델 파라미터 $\theta$를 선택하여 데이터의 우도를 최대화하는 방법입니다. 즉, 다음을 최대화합니다:</p>
            $$
            \theta^* = \arg\max_\theta \prod_{i=1}^N p_{model}(x^{(i)}; \theta)
            $$
            <p>로그를 취하면, 로그우도(log-likelihood)를 최대화하는 것과 같습니다:</p>
            $$
            \theta^* = \arg\max_\theta \sum_{i=1}^N \log p_{model}(x^{(i)}; \theta)
            $$
            <p>경험분포 $\hat{p}_{data}(x)$를 사용하면, 로그우도는 다음과 같이 쓸 수 있습니다:</p>
            $$
            \sum_{x} \hat{p}_{data}(x) \log p_{model}(x; \theta)
            $$
            <p>KL divergence의 정의를 다시 쓰면:</p>
            $$
            D_{KL}(\hat{p}_{data}(x) || p_{model}(x; \theta)) = \sum_{x} \hat{p}_{data}(x) \log \frac{\hat{p}_{data}(x)}{p_{model}(x; \theta)}
            $$
            <p>이를 전개하면,</p>
            $$
            D_{KL} = \sum_{x} \hat{p}_{data}(x) \log \hat{p}_{data}(x) - \sum_{x} \hat{p}_{data}(x) \log p_{model}(x; \theta)
            $$
            <p>여기서 첫 번째 항은 $\theta$와 무관하므로, KL divergence를 최소화하는 것은 두 번째 항을 최대화하는 것과 같습니다. 즉,</p>
            $$
            \arg\min_\theta D_{KL} = \arg\max_\theta \sum_{x} \hat{p}_{data}(x) \log p_{model}(x; \theta)
            $$
            <p>따라서, 최대우도추정(MLE)은 경험분포와 모델분포 사이의 KL divergence를 최소화하는 것과 동치입니다.</p>
        </div>
    </div>
</body>
</html>