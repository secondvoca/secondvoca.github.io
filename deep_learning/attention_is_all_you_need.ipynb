{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e5ad80",
   "metadata": {},
   "source": [
    "# Attention Is All You Need\n",
    "- [I Finally Understood “Attention is All You Need” After So Long. Here’s How I Did It.](https://ai.plainenglish.io/i-finally-understood-attention-is-all-you-need-after-so-long-heres-how-i-did-it-263b46273f9f)\n",
    "\n",
    "## 세 단계로 논문 읽기\n",
    "- 핵심 아이디어는 무엇인가?\n",
    "- 한 발짝 더 깊이\n",
    "- 세세한 기술 이해\n",
    "\n",
    "## 이 논문은 무엇을 해결한 것인가?\n",
    "- seq2seq 기술은 번역 문제처럼 sequence를 sequence로 매핑하는 기술이다. 이 전까지 RNN, LSTM의 방법은 입력과 출력의 과정이 앞 단어의 처리 후에 다음 단어를 처리하는 식이기 때문에 병렬화되지 못 했다. 이 논문에서는 attention mechanism만을 사용함으로써 (출력은 여전히 단계적이지만) 입력의 병렬처리가 가능하진다.\n",
    "\n",
    "## 모델의 구조\n",
    "- attention: f(query)->(key, value)\n",
    "- self-attention: Looks at words in the same sentence.\n",
    "- cross-attention: Decoder looks at encoder output.\n",
    "\n",
    "## scaled dot product attention\n",
    "$$Attention(Q,K,V)=softmax(QK^T/\\sqrt{d_k})\\cdot V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7c56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " [[1 0 1]]\n",
      "K:\n",
      " [[1 0 1]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n",
      "V:\n",
      " [[0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "\n",
      "d_k (dimension of K): 3\n",
      "\n",
      "Step 1: Compute scores (QK^T / sqrt(d_k)):\n",
      " [[1.15470054 0.         0.57735027]]\n",
      "\n",
      "Step 2: Softmax weights:\n",
      " [[0.53289684 0.16794345 0.29915971]]\n",
      "\n",
      "Step 3: Weighted sum (Attention Output):\n",
      " [[0.46710316 0.83205655]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def explain_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    print(\"Q:\\n\", Q)\n",
    "    print(\"K:\\n\", K)\n",
    "    print(\"V:\\n\", V)\n",
    "    print(\"\\nd_k (dimension of K):\", d_k)\n",
    "    \n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    print(\"\\nStep 1: Compute scores (QK^T / sqrt(d_k)):\\n\", scores)\n",
    "    \n",
    "    exp_scores = np.exp(scores)\n",
    "    \n",
    "    sum_exp_scores = np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    weights = exp_scores / sum_exp_scores\n",
    "    print(\"\\nStep 2: Softmax weights:\\n\", weights)\n",
    "    \n",
    "    output = np.dot(weights, V)\n",
    "    print(\"\\nStep 3: Weighted sum (Attention Output):\\n\", output)\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# Example input\n",
    "Q = np.array([[1, 0, 1]])\n",
    "K = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n",
    "V = np.array([[0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "output, weights = explain_attention(Q, K, V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
